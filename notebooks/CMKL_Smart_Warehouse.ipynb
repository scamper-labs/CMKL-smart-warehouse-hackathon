{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT7WvSKDzSy5"
      },
      "source": [
        "This is code that we use in Smart Warehouse Hackathon which contain:\n",
        "- Create Train datasets\n",
        "- Augmentation\n",
        "- Train\n",
        "- Predict\n",
        "    - pose precess\n",
        "    - combind and create submitsion file\n",
        "- Ensemble\n",
        "\n",
        "Note: We do those task separately but we try to write everything in one notebook. Consequently, it might has some error in directory because we didnot do everything by coding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7djwNKFW3la"
      },
      "source": [
        "Example videos:\n",
        "\n",
        "Short: https://drive.google.com/file/d/1urocaVfrtTtZGCTDlzOk8ybIs1Q2SR5H/view?usp=sharing\n",
        "\n",
        "Long: https://drive.google.com/file/d/15dy1OjI30Kyn5IF8_O5kM0G1FXB7GvCp/view?usp=sharing\n",
        "\n",
        "Note: Short video will compare normal detection and detection with motion detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "authors:\n",
        "- Patchara Opaspilai\n",
        "- Pitikorn Khlaisamniang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMvji9DLzXdR"
      },
      "source": [
        "# [1] Create Train Dataset Format for YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKidUC0OzTZa"
      },
      "outputs": [],
      "source": [
        "# We assume that you are already download and unzip train and test files\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "path_annotations_front_left = \"train/train/annotations/bangphli_front_left/\"\n",
        "path_annotations_front_center = \"train/train/annotations/bangphli_front_center/\"\n",
        "path_video_front_left = \"train/train/videos/bangphli_front_left/\"\n",
        "path_video_front_center = \"train/train/videos/bangphli_front_center/\"\n",
        "\n",
        "Path_annotations = [path_annotations_front_left, path_annotations_front_center]\n",
        "Path_videos = [path_video_front_left, path_video_front_center]\n",
        "\n",
        "for p in range(len(Path_annotations)):\n",
        "    path_annotations = Path_annotations[p]\n",
        "    path_videos = Path_videos[p]\n",
        "    for filename in os.listdir(path_annotations):\n",
        "        df = pd.read_csv(path_annotations + filename + \"/annotations.csv\")\n",
        "\n",
        "        # Save images\n",
        "        frame_list = []\n",
        "        for frame in df[\"frame\"]:\n",
        "            frame_list.append(frame)\n",
        "        video_path = path_videos + filename + \"/\" + filename + \".mp4\"\n",
        "        vidcap = cv2.VideoCapture(video_path)\n",
        "        success = True\n",
        "        width = vidcap.get(3)\n",
        "        height = vidcap.get(4)\n",
        "        count = 0\n",
        "        while success:\n",
        "            success, image = vidcap.read()\n",
        "            if count in frame_list:\n",
        "                try:\n",
        "                    cv2.imwrite(\n",
        "                        \"Train/images/\" + filename + \"_\" + str(count) + \".jpg\", image\n",
        "                    )  # save frame as JPEG file\n",
        "                except:\n",
        "                    print(filename + \"_\" + str(count))\n",
        "                    print(success)\n",
        "            count += 1\n",
        "\n",
        "        # Save labels\n",
        "        for i in range(len(df)):\n",
        "            txt = (\n",
        "                \"0 \"\n",
        "                + str(df[\"centroid_x\"][i] / width)\n",
        "                + \" \"\n",
        "                + str(df[\"centroid_y\"][i] / height)\n",
        "                + \" \"\n",
        "                + str(df[\"width\"][i] / width)\n",
        "                + \" \"\n",
        "                + str(df[\"height\"][i] / height)\n",
        "            )\n",
        "\n",
        "            with open(\"Train/labels/\" + filename + \"_\" + str(df[\"frame\"][i]) + \".txt\", \"w\") as f:\n",
        "                f.write(txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfcoMZ-d0RhE"
      },
      "source": [
        "# [2] Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl6v9nVn0Vq7"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "path = \"Train/\"\n",
        "\n",
        "image_list = os.listdir(path + \"images/\")\n",
        "\n",
        "os.mkdir(\"Results\")\n",
        "\n",
        "source_dir = path\n",
        "destination_dir = \"Results\"\n",
        "shutil.copytree(source_dir, destination_dir)\n",
        "C = 0\n",
        "for name in image_list:\n",
        "    if C % 2 == 0:\n",
        "        image = cv2.imread(path + \"images/\" + name)\n",
        "        w = image.shape[1]\n",
        "        h = image.shape[0]\n",
        "        txt = name[:-4] + \".txt\"\n",
        "        f = open(path + \"labels/\" + txt, \"r\")\n",
        "        bboxes = []\n",
        "        for x in f:\n",
        "            Class = str(x.split(\" \")[0])\n",
        "            X_c = float(x.split(\" \")[1])\n",
        "            Y_c = float(x.split(\" \")[2])\n",
        "            W = float(x.split(\" \")[3])\n",
        "            H = float(x.split(\" \")[4])\n",
        "            bboxes.append([X_c, Y_c, W, H, Class])\n",
        "\n",
        "        transform = A.Compose(\n",
        "            [\n",
        "                # A.RandomCrop(width=450, height=450),\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.RandomBrightnessContrast(p=0.7),\n",
        "            ],\n",
        "            bbox_params=A.BboxParams(format=\"yolo\"),\n",
        "        )\n",
        "\n",
        "        transformed = transform(image=image, bboxes=bboxes)\n",
        "        transformed_image = transformed[\"image\"]\n",
        "        transformed_bboxes = transformed[\"bboxes\"]\n",
        "\n",
        "        cv2.imwrite(destination_dir + \"/images/\" + name[:-4] + \"_2\" + \".jpg\", transformed_image)\n",
        "        labels = (\n",
        "            str(transformed_bboxes[0][4])\n",
        "            + \" \"\n",
        "            + str(transformed_bboxes[0][0])\n",
        "            + \" \"\n",
        "            + str(transformed_bboxes[0][1])\n",
        "            + \" \"\n",
        "            + str(transformed_bboxes[0][2])\n",
        "            + \" \"\n",
        "            + str(transformed_bboxes[0][3])\n",
        "        )\n",
        "\n",
        "        with open(destination_dir + \"/labels/\" + name[:-4] + \"_2\" + \".txt\", \"w\") as f:\n",
        "            f.write(labels)\n",
        "    C = C + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIwxAuHC1Hxz"
      },
      "source": [
        "# [3] Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eQ_FS375o8q"
      },
      "source": [
        "Create data.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiPEYF541LDF"
      },
      "outputs": [],
      "source": [
        "# We split data into 5 splits for train in 5 kfolds but we train only one split because there is no time\n",
        "\n",
        "# Create data.yaml\n",
        "\n",
        "import yaml\n",
        "\n",
        "data = \"\"\n",
        "\n",
        "with open(\"data.yaml\", \"w\") as outfile:\n",
        "    yaml.dump(data, outfile, default_flow_style=False)\n",
        "\n",
        "# Copy and paste this in yaml\n",
        "\"\"\"\n",
        "names:\n",
        "  0: 0\n",
        "path: Result\n",
        "train: train\n",
        "val: val\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbnuj53C5qeH"
      },
      "source": [
        "Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chuHSM7e5fUt"
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "\n",
        "ultralytics.checks()\n",
        "import datetime\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYM04CRh5uXD"
      },
      "outputs": [],
      "source": [
        "dataset_path = Path(\"Result\")  # replace with 'path/to/dataset' for your custom data\n",
        "labels = sorted(dataset_path.rglob(\"*labels/*.txt\"))  # all data in 'labels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85qx9ASs5vZk"
      },
      "outputs": [],
      "source": [
        "yaml_file = \"data.yaml\"  # your data YAML with data directories and names dictionary\n",
        "with open(yaml_file, \"r\", encoding=\"utf8\") as y:\n",
        "    classes = yaml.safe_load(y)[\"names\"]\n",
        "cls_idx = sorted(classes.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jevg19p5wWG"
      },
      "outputs": [],
      "source": [
        "indx = [l.stem for l in labels]  # uses base filename as ID (no extension)\n",
        "labels_df = pd.DataFrame([], columns=cls_idx, index=indx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUcdfaBS5xee"
      },
      "outputs": [],
      "source": [
        "for label in labels:\n",
        "    lbl_counter = Counter()\n",
        "\n",
        "    with open(label, \"r\") as lf:\n",
        "        lines = lf.readlines()\n",
        "\n",
        "    for l in lines:\n",
        "        # classes for YOLO label uses integer at first position of each line\n",
        "        lbl_counter[int(l.split(\" \")[0])] += 1\n",
        "\n",
        "    labels_df.loc[label.stem] = lbl_counter\n",
        "\n",
        "labels_df = labels_df.fillna(0.0)  # replace `nan` values with `0.0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-_LeqOX5yiN"
      },
      "outputs": [],
      "source": [
        "ksplit = 5\n",
        "kf = KFold(n_splits=ksplit, shuffle=True, random_state=20)  # setting random_state for repeatable results\n",
        "\n",
        "kfolds = list(kf.split(labels_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbfoeBpx5zUN"
      },
      "outputs": [],
      "source": [
        "folds = [f\"split_{n}\" for n in range(1, ksplit + 1)]\n",
        "folds_df = pd.DataFrame(index=indx, columns=folds)\n",
        "\n",
        "for idx, (train, val) in enumerate(kfolds, start=1):\n",
        "    folds_df[f\"split_{idx}\"].loc[labels_df.iloc[train].index] = \"train\"\n",
        "    folds_df[f\"split_{idx}\"].loc[labels_df.iloc[val].index] = \"val\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ierq8_Bg50s-"
      },
      "outputs": [],
      "source": [
        "fold_lbl_distrb = pd.DataFrame(index=folds, columns=cls_idx)\n",
        "\n",
        "for n, (train_indices, val_indices) in enumerate(kfolds, start=1):\n",
        "    train_totals = labels_df.iloc[train_indices].sum()\n",
        "    val_totals = labels_df.iloc[val_indices].sum()\n",
        "\n",
        "    # To avoid division by zero, we add a small value (1E-7) to the denominator\n",
        "    ratio = val_totals / (train_totals + 1e-7)\n",
        "    fold_lbl_distrb.loc[f\"split_{n}\"] = ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O24fczgm511p"
      },
      "outputs": [],
      "source": [
        "supported_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
        "\n",
        "# Initialize an empty list to store image file paths\n",
        "images = []\n",
        "\n",
        "# Loop through supported extensions and gather image files\n",
        "for ext in supported_extensions:\n",
        "    images.extend(sorted((dataset_path / \"images\").rglob(f\"*{ext}\")))\n",
        "\n",
        "# Create the necessary directories and dataset YAML files (unchanged)\n",
        "save_path = Path(dataset_path / f\"{datetime.date.today().isoformat()}_{ksplit}-Fold_Cross-val\")\n",
        "save_path.mkdir(parents=True, exist_ok=True)\n",
        "ds_yamls = []\n",
        "\n",
        "for split in folds_df.columns:\n",
        "    # Create directories\n",
        "    split_dir = save_path / split\n",
        "    split_dir.mkdir(parents=True, exist_ok=True)\n",
        "    (split_dir / \"train\" / \"images\").mkdir(parents=True, exist_ok=True)\n",
        "    (split_dir / \"train\" / \"labels\").mkdir(parents=True, exist_ok=True)\n",
        "    (split_dir / \"val\" / \"images\").mkdir(parents=True, exist_ok=True)\n",
        "    (split_dir / \"val\" / \"labels\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Create dataset YAML files\n",
        "    dataset_yaml = split_dir / f\"{split}_dataset.yaml\"\n",
        "    ds_yamls.append(dataset_yaml)\n",
        "\n",
        "    with open(dataset_yaml, \"w\") as ds_y:\n",
        "        yaml.safe_dump({\"path\": split_dir.as_posix(), \"train\": \"train\", \"val\": \"val\", \"names\": classes}, ds_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siMXHQZH522b"
      },
      "outputs": [],
      "source": [
        "for image, label in zip(images, labels):\n",
        "    for split, k_split in folds_df.loc[image.stem].items():\n",
        "        # Destination directory\n",
        "        img_to_path = save_path / split / k_split / \"images\"\n",
        "        lbl_to_path = save_path / split / k_split / \"labels\"\n",
        "\n",
        "        # Copy image and label files to new directory (SamefileError if file already exists)\n",
        "        shutil.copy(image, img_to_path / image.name)\n",
        "        shutil.copy(label, lbl_to_path / label.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G69umM1Q53_T"
      },
      "outputs": [],
      "source": [
        "folds_df.to_csv(save_path / \"kfold_datasplit.csv\")\n",
        "fold_lbl_distrb.to_csv(save_path / \"kfold_label_distribution.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa9Or8WU58fi"
      },
      "source": [
        "Train 3 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYF4sPvR6CU8"
      },
      "outputs": [],
      "source": [
        "# yolov8m\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"yolov8m.pt\")\n",
        "\n",
        "# Train the model\n",
        "results = model.train(\n",
        "    data=\"Result/2023-11-08_5-Fold_Cross-val/split_1/split_1_dataset.yaml\",\n",
        "    epochs=100,\n",
        "    imgsz=640,\n",
        "    batch=-1,\n",
        "    patience=20,\n",
        "    name=\"yolov8\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7j7w2ad6RdV"
      },
      "outputs": [],
      "source": [
        "# yolov5m (This model is for 1280 image size)\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"yolov5m6u.pt\")\n",
        "\n",
        "# Train the model\n",
        "results = model.train(\n",
        "    data=\"Result/2023-11-08_5-Fold_Cross-val/split_1/split_1_dataset.yaml\",\n",
        "    epochs=100,\n",
        "    imgsz=1280,\n",
        "    batch=-1,\n",
        "    patience=20,\n",
        "    name=\"yolov5\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIwPEquc61Kc"
      },
      "outputs": [],
      "source": [
        "# yolov6m\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"yolov6m.yaml\")\n",
        "\n",
        "# Train the model\n",
        "results = model.train(\n",
        "    data=\"Result/2023-11-08_5-Fold_Cross-val/split_1/split_1_dataset.yaml\",\n",
        "    epochs=100,\n",
        "    imgsz=640,\n",
        "    batch=-1,\n",
        "    patience=20,\n",
        "    name=\"yolov6\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNcoBPra7VUq"
      },
      "source": [
        "# [4] Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lme33DbV89SY"
      },
      "source": [
        "- We predict and create CSV for each videos and do pose process then combine all together.\n",
        "\n",
        "- In order to track the objects we use track form ultralytics. There will be a lot of box that get detect and track but not our targets. (Our targets is boxes that lifted by forklift) Therefore, we use motion detection to filter boxes that does not move by correct only id that pass the filter.\n",
        "- After predicr, we do pose process for clear all data that we dont want. For instance, id that we dont pass our filter. Moreover, rearange number of id.\n",
        "- Then we combine all csv together and use sequence_to_track file to create predicted_output by matching frame, video_name in combine_csv and sequence_to_track."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhxO_mzc9T-F"
      },
      "source": [
        "Create csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeZrPqXx7XFs"
      },
      "outputs": [],
      "source": [
        "import cv2, time\n",
        "from ultralytics import YOLO\n",
        "from shapely.geometry import Polygon\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8meM5hor74BY"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(box_1, box_2):\n",
        "    poly_1 = Polygon(box_1)\n",
        "    poly_2 = Polygon(box_2)\n",
        "    iou = poly_1.intersection(poly_2).area / poly_1.union(poly_2).area\n",
        "    return iou\n",
        "\n",
        "\n",
        "def box2box(boxes):\n",
        "    xmin = float(boxes[0])\n",
        "    ymin = float(boxes[1])\n",
        "    xmax = float(boxes[0]) + float(boxes[2])\n",
        "    ymax = float(boxes[1]) + float(boxes[3])\n",
        "\n",
        "    return [[xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]]\n",
        "\n",
        "\n",
        "def box2box_yolo(boxes):\n",
        "    xmin = float(boxes[0]) - float(boxes[2]) / 2\n",
        "    ymin = float(boxes[1]) - float(boxes[3]) / 2\n",
        "    xmax = float(boxes[0]) + float(boxes[2]) / 2\n",
        "    ymax = float(boxes[1]) + float(boxes[3]) / 2\n",
        "\n",
        "    return [[xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTrexzZm76T0"
      },
      "outputs": [],
      "source": [
        "# Predict ones per model\n",
        "\n",
        "model_name = \"yolov8m\"\n",
        "path = \"test/test/\"\n",
        "os.mkdir(\"csv\")\n",
        "os.mkdir(\"csv/\" + model_name)\n",
        "test_path = os.listdir(path)\n",
        "model = YOLO(\"runs/detect/\" + model_name + \"/weights/best.pt\")\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "Video_count = 0\n",
        "\n",
        "Done_list = []\n",
        "\n",
        "\n",
        "for test_floder in test_path:\n",
        "    type_path = os.listdir(path + test_floder + \"/test/videos/\")\n",
        "    for type in type_path:\n",
        "        video_path = path + test_floder + \"/test/videos/\" + type\n",
        "        videos = os.listdir(video_path)\n",
        "        for video in videos:\n",
        "            if Video_count not in Done_list:\n",
        "                # print(Video_count)\n",
        "                # print(video_path+'/'+video)\n",
        "                # print(type,video)\n",
        "                first_frame = None\n",
        "                check = True\n",
        "                Match = False\n",
        "                Old_COUNT = []\n",
        "                COUNT = []\n",
        "                COUNT_ID = []\n",
        "                Frame_count = 0\n",
        "                df = pd.DataFrame()\n",
        "                TYPE = []\n",
        "                VIDEO_NAME = []\n",
        "                ID = []\n",
        "                FRAME = []\n",
        "                XMIN = []\n",
        "                YMIN = []\n",
        "                XMAX = []\n",
        "                YMAX = []\n",
        "\n",
        "                Video = cv2.VideoCapture(video_path + \"/\" + video)\n",
        "\n",
        "                while check:\n",
        "                    check, frame = Video.read()\n",
        "                    if check == False:\n",
        "                        break\n",
        "                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "                    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
        "                    if first_frame is None:\n",
        "                        first_frame = gray\n",
        "                        continue\n",
        "                    delta_frame = cv2.absdiff(first_frame, gray)\n",
        "                    threshold_frame = cv2.threshold(delta_frame, 50, 255, cv2.THRESH_BINARY)[1]\n",
        "                    threshold_frame = cv2.dilate(threshold_frame, None, iterations=2)\n",
        "\n",
        "                    (cntr, _) = cv2.findContours(threshold_frame.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "                    Moving_area = []\n",
        "\n",
        "                    for contour in cntr:\n",
        "                        if cv2.contourArea(contour) < 1000:\n",
        "                            continue\n",
        "                        (x, y, w, h) = cv2.boundingRect(contour)\n",
        "                        Moving_area.append(box2box([x, y, w, h]))\n",
        "                        # cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),3)\n",
        "\n",
        "                    results = model.track(frame, persist=True, tracker=\"bytetrack.yaml\", imgsz=640)\n",
        "\n",
        "                    # Visualize the results on the frame\n",
        "                    annotated_frame = results[0].plot()\n",
        "\n",
        "                    boxes = results[0].boxes.xywh.cpu()\n",
        "                    track_ids = results[0].boxes.id\n",
        "\n",
        "                    for i in range(len(boxes)):\n",
        "                        Match = False\n",
        "                        M = 0\n",
        "                        for j in range(len(Moving_area)):\n",
        "                            if calculate_iou(box2box_yolo(boxes[i]), Moving_area[j]) > 0.0:\n",
        "                                Match = True\n",
        "                                M = M + calculate_iou(box2box_yolo(boxes[i]), Moving_area[j])\n",
        "\n",
        "                        if Match == True and M > 0.1:\n",
        "                            if track_ids == None:\n",
        "                                ids = \"-\"\n",
        "                            else:\n",
        "                                ids = int(track_ids[i])\n",
        "\n",
        "                                if len(COUNT) <= ids:\n",
        "                                    for k in range(ids - len(COUNT) + 1):\n",
        "                                        COUNT.append(0)\n",
        "                                    for l in range(ids - len(COUNT_ID) + 1):\n",
        "                                        COUNT_ID.append(0)\n",
        "                                COUNT[ids] = COUNT[ids] + 1\n",
        "\n",
        "                            xmin = float(boxes[i][0]) - float(boxes[i][2]) / 2\n",
        "                            ymin = float(boxes[i][1]) - float(boxes[i][3]) / 2\n",
        "                            xmax = float(boxes[i][0]) + float(boxes[i][2]) / 2\n",
        "                            ymax = float(boxes[i][1]) + float(boxes[i][3]) / 2\n",
        "                            # print(ids)\n",
        "                            if ids == \"-\":\n",
        "                                color = (0, 0, 255)\n",
        "                            elif COUNT[ids] < 15:\n",
        "                                color = (0, 0, 255)\n",
        "                            else:\n",
        "                                color = (0, 255, 0)\n",
        "                                COUNT_ID[ids] = 1\n",
        "                            cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, 3)\n",
        "\n",
        "                            frame = cv2.putText(\n",
        "                                frame,\n",
        "                                \"ID\" + str(ids) + \"_\" + str(M),\n",
        "                                (int(xmin), int(ymin) + 5),\n",
        "                                font,\n",
        "                                2,\n",
        "                                color,\n",
        "                                5,\n",
        "                                cv2.LINE_AA,\n",
        "                            )\n",
        "                            TYPE.append(type)\n",
        "                            VIDEO_NAME.append(video)\n",
        "                            ID.append(ids)\n",
        "                            FRAME.append(Frame_count)\n",
        "                            XMIN.append(int(xmin))\n",
        "                            YMIN.append(int(ymin))\n",
        "                            XMAX.append(int(xmax))\n",
        "                            YMAX.append(int(ymax))\n",
        "\n",
        "                    if Frame_count % 100 == 0:\n",
        "                        for z in range(len(Old_COUNT)):\n",
        "                            if Old_COUNT[z] == COUNT[z] and COUNT[z] < 15:\n",
        "                                COUNT[z] = 0\n",
        "\n",
        "                        Old_COUNT = COUNT\n",
        "\n",
        "                    if Frame_count % 1000 == 0:\n",
        "                        df = pd.DataFrame()\n",
        "                        df[\"type\"] = TYPE\n",
        "                        df[\"name\"] = VIDEO_NAME\n",
        "                        df[\"id\"] = ID\n",
        "                        df[\"frame\"] = FRAME\n",
        "                        df[\"xmin\"] = XMIN\n",
        "                        df[\"ymin\"] = YMIN\n",
        "                        df[\"xmax\"] = XMAX\n",
        "                        df[\"ymax\"] = YMAX\n",
        "                        df.to_csv(\"watch.csv\", index=False)\n",
        "                    print(COUNT)\n",
        "                    print(COUNT_ID)\n",
        "                    Ans = []\n",
        "                    for m in range(len(COUNT_ID)):\n",
        "                        if COUNT_ID[m] == 1:\n",
        "                            Ans.append(m)\n",
        "                    print(Ans)\n",
        "                    frame = cv2.putText(frame, str(Ans), (0, 100), font, 2, (255, 0, 0), 5, cv2.LINE_AA)\n",
        "                    # cv2.imshow(\"YOLOv8 Tracking\", frame)\n",
        "                    Frame_count = Frame_count + 1\n",
        "                    # cv2.imshow(\"frame\",frame)\n",
        "                    # key=cv2.waitKey(1)\n",
        "                    # if key==ord('q'):\n",
        "                    #    break\n",
        "\n",
        "                Video.release()\n",
        "                cv2.destroyAllWindows()\n",
        "                df = pd.DataFrame()\n",
        "                df[\"type\"] = TYPE\n",
        "                df[\"name\"] = VIDEO_NAME\n",
        "                df[\"id\"] = ID\n",
        "                df[\"frame\"] = FRAME\n",
        "                df[\"xmin\"] = XMIN\n",
        "                df[\"ymin\"] = YMIN\n",
        "                df[\"xmax\"] = XMAX\n",
        "                df[\"ymax\"] = YMAX\n",
        "\n",
        "                for y in range(len(COUNT)):\n",
        "                    if COUNT_ID[y] == 0:\n",
        "                        df = df.drop(df[df[\"id\"] == y].index)\n",
        "\n",
        "                for i in range(len(Ans)):\n",
        "                    df.loc[df[\"id\"] == Ans[i], \"id\"] = i\n",
        "\n",
        "                df = df.drop(df[df[\"id\"] == \"-\"].index)\n",
        "\n",
        "                df.to_csv(\"csv/\" + model_name + \"/\" + str(Video_count) + \".csv\", index=False)\n",
        "\n",
        "            Video_count = Video_count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZB41MB-_y-4"
      },
      "source": [
        "Pose precess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gaZWUKW_wwr"
      },
      "outputs": [],
      "source": [
        "# replace all csv by Pose precess csv\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "model_name = \"yolov8m\"\n",
        "\n",
        "path = \"csv/\" + model_name + \"/\"\n",
        "\n",
        "csv_path = os.listdir(path)\n",
        "\n",
        "\n",
        "for i in range(len(csv_path)):\n",
        "    df_csv = pd.read_csv(path + str(i) + \".csv\")\n",
        "    Del = []\n",
        "    Start = -1\n",
        "    for j in range(len(df_csv)):\n",
        "        if df_csv[\"id\"][j] == Start or df_csv[\"id\"][j] == Start + 1:\n",
        "            Del.append(0)\n",
        "            Start = df_csv[\"id\"][j]\n",
        "        else:\n",
        "            Del.append(1)\n",
        "    df_csv[\"Del\"] = Del\n",
        "    df_csv = df_csv.drop(df_csv[df_csv[\"Del\"] == 1].index)\n",
        "    df_csv = df_csv.drop(columns=[\"Del\"])\n",
        "    df_csv.reset_index(inplace=True)\n",
        "    MAX = max(df_csv[\"id\"])\n",
        "    Dis = []\n",
        "    for k in range(MAX + 1):\n",
        "        First = True\n",
        "        for l in range(len(df_csv)):\n",
        "            if df_csv[\"id\"][l] == k:\n",
        "                if First == True:\n",
        "                    first = [df_csv[\"xmax\"][l] - df_csv[\"xmin\"][l], df_csv[\"ymax\"][l] - df_csv[\"ymin\"][l]]\n",
        "                    First = False\n",
        "\n",
        "                last = [df_csv[\"xmax\"][l] - df_csv[\"xmin\"][l], df_csv[\"ymax\"][l] - df_csv[\"ymin\"][l]]\n",
        "        distance = abs(last[0] - first[0]) ** 2 + abs(last[1] - first[1]) ** 2\n",
        "        Dis.append(distance)\n",
        "\n",
        "    Dis.reverse()\n",
        "    for m in reversed(range(len(Dis))):\n",
        "        distance = Dis[MAX - m]\n",
        "        if distance <= 1000:\n",
        "            df_csv = df_csv.drop(df_csv[df_csv[\"id\"] == m].index)\n",
        "\n",
        "            for n in range(MAX - m):\n",
        "                # print((str(MAX-n)+' to ' + str(MAX-n-1)))\n",
        "                df_csv.loc[df_csv[\"id\"] == MAX - n, \"id\"] = MAX - n - 1\n",
        "\n",
        "    df_csv.reset_index(inplace=True)\n",
        "\n",
        "    df_csv = df_csv.drop(columns=[\"index\", \"level_0\"])\n",
        "    df_csv.to_csv(\"csv/\" + model_name + \"/\" + str(i) + \".csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0HyxdeqAUJR"
      },
      "source": [
        "Combine csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scwnv0NQAdP9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "model_name = \"yolov8m\"\n",
        "\n",
        "path = \"csv/\" + model_name + \"/\"\n",
        "\n",
        "csv_path = os.listdir(path)\n",
        "\n",
        "frames = []\n",
        "\n",
        "for i in range(len(csv_path)):\n",
        "    df_csv = pd.read_csv(path + str(i) + \".csv\")\n",
        "    frames.append(df_csv)\n",
        "\n",
        "result = pd.concat(frames)\n",
        "\n",
        "result.to_csv(model_name + \".csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqWTT5y8A4SB"
      },
      "source": [
        "fin boxes column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhJ0hmUlBFbU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "model_name = \"yolov8m\"\n",
        "\n",
        "df = pd.read_csv(\"sequence_to_track.csv\")\n",
        "df_ans = pd.read_csv(model_name + \".csv\")\n",
        "\n",
        "xmin = []\n",
        "ymin = []\n",
        "xmax = []\n",
        "ymax = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    Find = False\n",
        "    check = df[\"name\"][i] + str(\".mp4\")\n",
        "    df_check = df_ans.loc[(df_ans[\"name\"] == check)]\n",
        "    df_check = df_check.reset_index()\n",
        "    for j in range(len(df_check)):\n",
        "        if df[\"frame\"][i] == df_check[\"frame\"][j]:\n",
        "            Find = True\n",
        "            Xmin = df_check[\"xmin\"][j]\n",
        "            Ymin = df_check[\"ymin\"][j]\n",
        "            Xmax = df_check[\"xmax\"][j] - df_check[\"xmin\"][j]\n",
        "            Ymax = df_check[\"ymax\"][j] - df_check[\"ymin\"][j]\n",
        "\n",
        "    if Find == True:\n",
        "        xmin.append(Xmin)\n",
        "        ymin.append(Ymin)\n",
        "        xmax.append(Xmax)\n",
        "        ymax.append(Ymax)\n",
        "\n",
        "    else:\n",
        "        if df[\"xmin\"][i] == -1:\n",
        "            xmin.append(-1)\n",
        "            ymin.append(-1)\n",
        "            xmax.append(-1)\n",
        "            ymax.append(-1)\n",
        "        else:\n",
        "            xmin.append(df[\"xmin\"][i])\n",
        "            ymin.append(df[\"ymin\"][i])\n",
        "            xmax.append(df[\"xmax\"][i])\n",
        "            ymax.append(df[\"ymax\"][i])\n",
        "\n",
        "\n",
        "df[\"xmin\"] = xmin\n",
        "df[\"ymin\"] = ymin\n",
        "df[\"xmax\"] = xmax\n",
        "df[\"ymax\"] = ymax\n",
        "\n",
        "\n",
        "df.to_csv(model_name + \"_before.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKqUjODlBxP2"
      },
      "source": [
        "Another pose precess\n",
        "- These are some frames that we cant detect, so this pose precess will try to fill those blank by calculate from near by boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdOG-G2tB18j"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "model_name = \"yolov8m\"\n",
        "\n",
        "df = pd.read_csv(model_name + \"_before.csv\")\n",
        "\n",
        "List = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    Save = df[\"type\"][i] + \",\" + df[\"name\"][i]\n",
        "\n",
        "    if Save not in List:\n",
        "        List.append(Save)\n",
        "\n",
        "C = 0\n",
        "xmin = []\n",
        "ymin = []\n",
        "xmax = []\n",
        "ymax = []\n",
        "for i in range(len(List)):\n",
        "    Type = List[i].split(\",\")[0]\n",
        "    Name = List[i].split(\",\")[1]\n",
        "\n",
        "    df_video = df.loc[(df[\"name\"] == Name) & (df[\"type\"] == Type)]\n",
        "\n",
        "    df_video = df_video.reset_index()\n",
        "\n",
        "    List_id = []\n",
        "    for z in range(len(df_video)):\n",
        "        Save = df_video[\"id\"][z]\n",
        "        if Save not in List_id:\n",
        "            List_id.append(Save)\n",
        "\n",
        "    for x in range(len(List_id)):\n",
        "        df_check = df_video.loc[(df_video[\"id\"] == x)]\n",
        "        df_check = df_check.reset_index()\n",
        "\n",
        "        List_id_2 = []\n",
        "        for z in range(len(df_check)):\n",
        "            Save = df_check[\"xmin\"][z]\n",
        "            if Save not in List_id_2:\n",
        "                List_id_2.append(Save)\n",
        "\n",
        "        if len(List_id_2) == 2:\n",
        "            for j in range(len(df_check)):\n",
        "                xmin.append(df_check[\"xmin\"][0])\n",
        "                ymin.append(df_check[\"ymin\"][0])\n",
        "                xmax.append(df_check[\"xmax\"][0])\n",
        "                ymax.append(df_check[\"ymax\"][0])\n",
        "\n",
        "        else:\n",
        "            for j in range(len(df_check)):\n",
        "                if df_check[\"xmin\"][j] == -1:\n",
        "                    Second = False\n",
        "                    Length = 0\n",
        "                    for k in range(len(df_check) - j - 1):\n",
        "                        if df_check[\"xmin\"][j + k + 1] == -1:\n",
        "                            pass\n",
        "                        else:\n",
        "                            Second = df_check[\"xmin\"][j + k + 1]\n",
        "                            Length = k + 1\n",
        "                            break\n",
        "                    if Second == False:\n",
        "                        F_xmin = df_check[\"xmin\"][j - 2]\n",
        "                        F_ymin = df_check[\"ymin\"][j - 2]\n",
        "                        F_xmax = df_check[\"xmax\"][j - 2]\n",
        "                        F_ymax = df_check[\"ymax\"][j - 2]\n",
        "\n",
        "                        S_xmin = df_check[\"xmin\"][j - 1]\n",
        "                        S_ymin = df_check[\"ymin\"][j - 1]\n",
        "                        S_xmax = df_check[\"xmax\"][j - 1]\n",
        "                        S_ymax = df_check[\"ymax\"][j - 1]\n",
        "\n",
        "                        xmin.append(int(S_xmin + (S_xmin - F_xmin)))\n",
        "                        ymin.append(int(S_ymin + (S_ymin - F_ymin)))\n",
        "                        xmax.append(int(S_xmax + (S_xmax - F_xmax)))\n",
        "                        ymax.append(int(S_ymax + (S_ymax - F_ymax)))\n",
        "\n",
        "                        df_check.loc[j, \"xmin\"] = int(S_xmin + (S_xmin - F_xmin))\n",
        "                        df_check.loc[j, \"ymin\"] = int(S_ymin + (S_ymin - F_ymin))\n",
        "                        df_check.loc[j, \"xmax\"] = int(S_xmax + (S_xmax - F_xmax))\n",
        "                        df_check.loc[j, \"ymax\"] = int(S_ymax + (S_ymax - F_ymax))\n",
        "\n",
        "                    else:\n",
        "                        F_xmin = df_check[\"xmin\"][j - 1]\n",
        "                        F_ymin = df_check[\"ymin\"][j - 1]\n",
        "                        F_xmax = df_check[\"xmax\"][j - 1]\n",
        "                        F_ymax = df_check[\"ymax\"][j - 1]\n",
        "\n",
        "                        S_xmin = df_check[\"xmin\"][j + Length]\n",
        "                        S_ymin = df_check[\"ymin\"][j + Length]\n",
        "                        S_xmax = df_check[\"xmax\"][j + Length]\n",
        "                        S_ymax = df_check[\"ymax\"][j + Length]\n",
        "\n",
        "                        xmin.append(int(F_xmin + ((S_xmin - F_xmin) / (Length + 1))))\n",
        "                        ymin.append(int(F_ymin + ((S_ymin - F_ymin) / (Length + 1))))\n",
        "                        xmax.append(int(F_xmax + ((S_xmax - F_xmax) / (Length + 1))))\n",
        "                        ymax.append(int(F_ymax + ((S_ymax - F_ymax) / (Length + 1))))\n",
        "\n",
        "                        df_check.loc[j, \"xmin\"] = F_xmin + ((S_xmin - F_xmin) / (Length + 1))\n",
        "                        df_check.loc[j, \"ymin\"] = F_ymin + ((S_ymin - F_ymin) / (Length + 1))\n",
        "                        df_check.loc[j, \"xmax\"] = F_xmax + ((S_xmax - F_xmax) / (Length + 1))\n",
        "                        df_check.loc[j, \"ymax\"] = F_ymax + ((S_ymax - F_ymax) / (Length + 1))\n",
        "\n",
        "                else:\n",
        "                    xmin.append(df_check[\"xmin\"][j])\n",
        "                    ymin.append(df_check[\"ymin\"][j])\n",
        "                    xmax.append(df_check[\"xmax\"][j])\n",
        "                    ymax.append(df_check[\"ymax\"][j])\n",
        "\n",
        "df[\"xmin\"] = xmin\n",
        "df[\"ymin\"] = ymin\n",
        "df[\"xmax\"] = xmax\n",
        "df[\"ymax\"] = ymax\n",
        "\n",
        "df.to_csv(model_name + \"_after.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQcZ_N9kCl-7"
      },
      "source": [
        "Just make every answer to int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HosS45AWClMQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "model_name = \"yolov8m\"\n",
        "\n",
        "df = pd.read_csv(model_name + \"_after.csv\")\n",
        "\n",
        "xmin = []\n",
        "ymin = []\n",
        "xmax = []\n",
        "ymax = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    xmin.append(int(df[\"xmin\"][i]))\n",
        "    ymin.append(int(df[\"ymin\"][i]))\n",
        "    xmax.append(int(df[\"xmax\"][i]))\n",
        "    ymax.append(int(df[\"ymax\"][i]))\n",
        "\n",
        "df[\"xmin\"] = xmin\n",
        "df[\"ymin\"] = ymin\n",
        "df[\"xmax\"] = xmax\n",
        "df[\"ymax\"] = ymax\n",
        "\n",
        "df.to_csv(model_name + \"_after.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSAibz74CyPz"
      },
      "source": [
        "Note: We create predicted_output by copy after.csv and paste into txt file and del column name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odTlvbbgDH6U"
      },
      "source": [
        "# [5] Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYM_ALDJDO78"
      },
      "source": [
        "Combine result of all 3 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkeFYDptDHKg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "df_1 = pd.read_csv(\"yolov5_before.csv\")\n",
        "df_2 = pd.read_csv(\"yolov8_before.csv\")\n",
        "df_3 = pd.read_csv(\"yolov6_before.csv\")\n",
        "\n",
        "xmin = []\n",
        "ymin = []\n",
        "xmax = []\n",
        "ymax = []\n",
        "\n",
        "for i in range(len(df_1)):\n",
        "    if df_1[\"xmin\"][i] != -1:\n",
        "        xmin.append(int(df_1[\"xmin\"][i]))\n",
        "        ymin.append(int(df_1[\"ymin\"][i]))\n",
        "        xmax.append(int(df_1[\"xmax\"][i]))\n",
        "        ymax.append(int(df_1[\"ymax\"][i]))\n",
        "    elif df_2[\"xmin\"][i] != -1:\n",
        "        xmin.append(int(df_2[\"xmin\"][i]))\n",
        "        ymin.append(int(df_2[\"ymin\"][i]))\n",
        "        xmax.append(int(df_2[\"xmax\"][i]))\n",
        "        ymax.append(int(df_2[\"ymax\"][i]))\n",
        "    else:\n",
        "        xmin.append(int(df_3[\"xmin\"][i]))\n",
        "        ymin.append(int(df_3[\"ymin\"][i]))\n",
        "        xmax.append(int(df_3[\"xmax\"][i]))\n",
        "        ymax.append(int(df_3[\"ymax\"][i]))\n",
        "\n",
        "df_1[\"xmin\"] = xmin\n",
        "df_1[\"ymin\"] = ymin\n",
        "df_1[\"xmax\"] = xmax\n",
        "df_1[\"ymax\"] = ymax\n",
        "\n",
        "df_1.to_csv(\"ens_1_before.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
